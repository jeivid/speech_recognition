{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(file_path, chunk_length_ms=30000, overlap_ms=1000):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    end = chunk_length_ms\n",
    "\n",
    "    while start < len(audio):\n",
    "        chunk = audio[start:end]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Increment start by chunk length minus overlap\n",
    "        start += chunk_length_ms - overlap_ms\n",
    "        end = start + chunk_length_ms\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_chunks(chunks, model_name='base'):\n",
    "    model = whisper.load_model(model_name)\n",
    "    transcriptions = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Export the chunk to a temporary file\n",
    "        chunk_file = f\"temp_chunk_{i}.wav\"\n",
    "        chunk.export(chunk_file, format=\"wav\")\n",
    "\n",
    "        # Transcribe the audio chunk using Whisper\n",
    "        result = model.transcribe(chunk_file)\n",
    "        transcriptions.append(result[\"text\"])\n",
    "\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_temp_files(chunks):\n",
    "    for i in range(len(chunks)):\n",
    "        os.remove(f\"temp_chunk_{i}.wav\")\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_chunks = split_audio(\"Akhundzade3.mp3\")\n",
    "audio_chunks = audio_chunks[0:2]\n",
    "transcriptions = transcribe_chunks(audio_chunks)\n",
    "full_transcription = ' '.join(transcriptions)\n",
    "print(full_transcription)\n",
    "clean_up_temp_files(audio_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a kind assistant, skilled in transforming poorly written farsi into proper formal language, without mistakes and misunderstandings. You avoid adding comments, you just rewrite the user input and do not add any comment.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{full_transcription}\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
